foundation_models:
  phobert:
    model_name: "vinai/phobert-base-v2"
    tokenizer_name: "vinai/phobert-base-v2"
    embedding_dim: 768
    type: "transformer"
  visobert:
    model_name: "uitnlp/visobert"
    tokenizer_name: "uitnlp/visobert"
    embedding_dim: 768
    type: "transformer"
  vit5:
    model_name: "VietAI/vit5-base"
    tokenizer_name: "VietAI/vit5-base"
    embedding_dim: 768
    type: "transformer"

# Legacy support for backward compatibility
emotion_knowledge_base:
  emoticon_sentiment: "https://huggingface.co/datasets/viethq1906/emotion-symbols-sentiment/resolve/main/emoticon_sentiment.csv"
  emoji_sentiment: "https://huggingface.co/datasets/viethq1906/emotion-symbols-sentiment/resolve/main/esr.csv" 

training_setting:
  output_dir: "./experiment/outputs_final"
  output_structure: "method/dataset/model_name/head_model"
  max_length: 30
  lora_lr: 0.00002
  head_lr: 0.0003
  batch_train: 32
  batch_eval: 64
  epochs: 20
  weight_decay: 0.03
  warmup_ratio: 0.15
  warmup_steps: 0
  param_groups: ["lora","head"]
  fp16: true
  max_grad_norm: 0.5
  early_stopping:
    metric: eval_loss
    patience: 30
  best_model_metric: accuracy  # Metric used for saving best model
  lora:
    r: 16
    alpha: 16
    dropout: 0.1
    bias: "none"
    target_modules: ["query", "key", "value"]
  multi_task_learning:
    alpha: 0.3
    beta: 0.7

model:
  meta_cls:
    input_dim: 15
    hidden_dim: 32
    output_dim: 3
    learning_rate: 0.0003
  lstm:
    hidden_size: 128
    num_layers: 1
    dropout: 0.4
    out_dim: 3
  gru:
    hidden_size: 128
    num_layers: 1
    dropout: 0.4
    out_dim: 3
  bigru:
    hidden_size: 96   # per direction -> 192 concat
    num_layers: 1
    dropout: 0.4
    out_dim: 3
  bilstm:
    hidden_size: 96   # per direction -> 192 concat
    num_layers: 1
    dropout: 0.4
    out_dim: 3
  cnn:
    num_filters: 128
    kernel_sizes: [2,3,4]
    dropout: 0.3
    out_dim: 3
  logreg:
    out_dim: 3
    dropout: 0.1
  xgboost:
    max_depth: 4
    n_estimators: 200
    learning_rate: 0.1
    subsample: 0.9
    colsample_bytree: 0.9
  loss:
    label_smoothing: 0.02